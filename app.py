import streamlit as st
import pandas as pd
import openai, io, re
import fitz                # PyMuPDF
from PIL import Image
import base64

# --- 0Ô∏è‚É£ Page config & style ---
st.set_page_config(page_title="Fiche de r√©ception", layout="wide", page_icon="üìã")
st.markdown("""
<style>
  .card { background:white; padding:1.5rem; border-radius:0.5rem;
          box-shadow:0 4px 6px rgba(0,0,0,0.1); margin-bottom:2rem;}
  .section-title { font-size:1.6rem; color:#4A90E2; margin-bottom:0.5rem;}
  .debug { background:#f0f0f0; padding:0.5rem; border-radius:0.25rem; font-family:monospace;}
</style>
""", unsafe_allow_html=True)
st.markdown(
  '<div class="section-title">üì• Documents de r√©ception ‚Üí FICHE DE R√âCEPTION</div>',
  unsafe_allow_html=True
)

# --- 1Ô∏è‚É£ Cl√© OpenAI & init ---
OPENAI_API_KEY = st.secrets.get("OPENAI_API_KEY", "")
if not OPENAI_API_KEY:
    st.error("üõë D√©finis `OPENAI_API_KEY` dans les Secrets.")
    st.stop()
openai.api_key = OPENAI_API_KEY

# --- 2Ô∏è‚É£ Test rapide de l‚ÄôAPI OpenAI OCR (GPT-4V) ---
if st.button("üõ†Ô∏è Tester OCR OpenAI"):
    # G√©n√®re une image avec le mot HELLO
    img = Image.new("RGB",(200,60),"white")
    from PIL import ImageDraw
    d = ImageDraw.Draw(img)
    d.text((10,10),"HELLO","black")
    buf = io.BytesIO(); img.save(buf,format="PNG")
    b = buf.getvalue()
    # Envoie √† GPT-4 Vision via ChatCompletion
    resp = openai.ChatCompletion.create(
      model="gpt-4o-mini",       # ou "gpt-4-vision-preview"
      messages=[
        {"role":"system","content":"Tu es un OCR pr√©cis. Retourne juste le texte brut."},
        {"role":"user","content":"Extract text from this image."}
      ],
      functions=[{
        "name":"extract_text",
        "parameters":{"type":"object","properties":{"image_base64":{"type":"string"}},"required":["image_base64"]}
      }],
      function_call={"name":"extract_text","arguments":f'{{"image_base64":"{base64.b64encode(b).decode()}"}}'}
    )
    txt = resp["choices"][0]["message"]["content"]
    st.write("üîç Texte d√©tect√© :", repr(txt))
    st.stop()

# --- 3Ô∏è‚É£ Fonctions utilitaires OCR & parsing ---

def ocr_openai_image(img_bytes: bytes) -> str:
    """
    Envoie l‚Äôimage (base64) √† GPT-4 Vision, r√©cup√©ration du texte brut.
    """
    b64 = base64.b64encode(img_bytes).decode()
    res = openai.ChatCompletion.create(
      model="gpt-4o-mini",       # remplacer par "gpt-4-vision-preview" si dispo
      messages=[
        {"role":"system","content":"Tu es un OCR pr√©cis. Retourne juste le texte brut, ligne par ligne."},
        {"role":"user","content":"Extract text from the provided image."}
      ],
      functions=[{
        "name":"extract_text",
        "parameters":{"type":"object","properties":{"image_base64":{"type":"string"}},"required":["image_base64"]}
      }],
      function_call={"name":"extract_text","arguments":f'{{"image_base64":"{b64}"}}'}
    )
    return res["choices"][0]["message"]["content"]

def pdf_to_image(pdf_bytes: bytes) -> Image.Image:
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    pix = doc[0].get_pixmap(dpi=300)
    return Image.open(io.BytesIO(pix.tobytes("png")))

def read_excel(buf: bytes) -> pd.DataFrame:
    df = pd.read_excel(io.BytesIO(buf))
    df = df.rename(columns={
      df.columns[0]:"R√©f√©rence",
      df.columns[1]:"Nb de colis",
      df.columns[2]:"pcs par colis"
    })
    df["total"] = df["Nb de colis"]*df["pcs par colis"]
    df["V√©rification"] = ""
    return df[["R√©f√©rence","Nb de colis","pcs par colis","total","V√©rification"]]

def parse_with_fallback(raw: str) -> pd.DataFrame:
    # Ton parsing robuste existant ici (parse_robust, parse_generic, parse_sequential)
    # ‚Ä¶
    return df  # r√©sultat du meilleur des 3 fallback

def ocr_by_columns_with_fallback(img: Image.Image) -> pd.DataFrame:
    w,h = img.size
    cuts = [0.3,0.6]
    boxes = [
      (0,0,int(w*cuts[0]),h),
      (int(w*cuts[0]),0,int(w*cuts[1]),h),
      (int(w*cuts[1]),0,w,h)
    ]
    zone_lines, counts = [], []
    for idx,(x1,y1,x2,y2) in enumerate(boxes,1):
        crop = img.crop((x1,y1,x2,y2))
        buf = io.BytesIO(); crop.save(buf,format="PNG")
        txt = ocr_openai_image(buf.getvalue())
        lines = [l.strip() for l in txt.splitlines() if l.strip()]
        zone_lines.append(lines)
        counts.append(len(lines))
        st.markdown(f"**Zone {idx} brut:**")
        st.markdown(f"<div class='debug'>{txt or '(vide)'}</div>",unsafe_allow_html=True)
    st.write(f"üìä Lignes d√©tect√©es: R√©f={counts[0]}, Colis={counts[1]}, Pi√®ces={counts[2]}")
    n = min(*counts)
    if n>0:
      # Construire df comme avant‚Ä¶
      return df
    st.warning("‚ö†Ô∏è Aucune zone align√©e ; fallback page enti√®re‚Ä¶")
    full = ocr_openai_image(img_to_bytes(img))
    st.subheader("üîç Texte complet OCR")
    st.text_area("", full or "(vide)", height=300)
    return parse_with_fallback(full)

# --- 4Ô∏è‚É£ Interface & workflow 4 conteneurs (Import ‚Üí Extraction ‚Üí R√©sultats ‚Üí Export) ---
# √Ä copier-coller tel quel depuis ta version pr√©c√©dente, 
# en rempla√ßant simplement les appels √† Google Vision par ocr_openai_image / ocr_by_columns_with_fallback.
